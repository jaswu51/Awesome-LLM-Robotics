
Datasets: 
* The RT2 paper utilized the same datasets as RT1, thus it still suffers that we will obtain a massive dataset for each robot.

Network Structure:
  
* Instead of training a vanilla transformer from scratch, RT2 directly fine-tuned an existing Vision-Language Model (VLM) such as PaLM-E. 
  
* The output of the RT2 model remained text tokens, but each action choice was mapped to a specific token. For instance, the x-axis transformation of the robot gripper was classified into 256 bins, requiring 256 tokens to generate an x-axis position action choice. In total, only 256 of the 1000 output text tokens were needed to represent the robotic actions. 

Red Flags - Inference Speed:

* RT2 paper didn't mention its precise inference speed but stated it in the limitations, which in RT1 it is about 1.9 seconds per inference. Since the RT2 model parameter size is 50B, compared to RT1's 1B, it may take 50 times more time than RT1. The paper suggest to tackle the real-time inference by quantization and distillation techniques. 